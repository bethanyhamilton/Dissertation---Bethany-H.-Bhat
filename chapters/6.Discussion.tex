\chapter{Discussion}\label{ch: discussion}

In meta-analysis, multiple dependent effect size estimates in primary studies are prevalent, and methods that account for the dependence are widely used \autocite{hedges2010, pustejovsky2022, vandennoortgate2013, vandennoortgate2015, betsy_jane_becker_model}. A primary goal of meta-analysis is to explain heterogeneity of effect sizes through meta-regression. One way to do this is to include categorical moderators, a common type of covariate in social science meta-analytic research \autocite{tipton2019a}. With the development and wide use of working models that account for more than one source of dependence to be used with RVE meta-regression \autocite{pustejovsky2022}, a methodology for conducting power analysis for the tests of such models is needed for obtaining \textit{a priori} power estimates. Due to the complexity of these models, a larger number of studies is needed than for models for independent effect sizes, so it is important to consider performance of these models when planning a systematic review. Furthermore, using univariate \textit{a priori} power analysis methods \autocite{hedges2001} for a meta-analysis involving dependent effect sizes results in inaccurate power estimates \autocite{vembye2023}.  

\textcite{vembye2023} developed and evaluated approximation formulas for the power of the test of the overall pooled effect for the MLMA, CE, and CHE working models. I extended this work in my dissertation by developing and evaluating a closed-form approximation for the HTZ tests of study-level categorical moderators for the CHE model.  Below, I first discuss the implications of the results I found and then I detail the limitations of this study and offer future directions. 


\section{Implications}

% the primary conclusion is that the approximation is accurate except for high number of categories and small degrees of freedom. Something that can calculated in advanced. Do power calculation in advanced and look at the degrees of freedom, if very small than maybe don't trust the result.


I had three aims for my study: 1) To validate the approximation for the power of the Wald test of a categorical study-level moderator from the CHE model with HTZ degrees of freedom through a simulation; 2) To compare methods for assuming sampling variances and the number of effect sizes in the power approximation formula; and 3) To evaluate Type I Error for the robust-Wald Test from a CHE working model. From the results of this study, I conclude that 1) The power approximation formula is accurate when there are a small number of contrasts, but it can be inaccurate in conditions with a larger number of contrasts and small degrees of freedom; 2) It is important to find reliable pilot data for sampling the number of the effect sizes and the sampling variances, as the accuracy of the formula did depended on how closely the assumed distributions matched the data generating distributions; and 3) The Type I Error of the robust-Wald Test from a CHE working model was below the nominal error rate, especially in conditions with more contrasts and a small number of total studies. I expand on these conclusions below.  

For the first aim, under an ideal situation where the primary study characteristics for the power approximation were sampled from the same dataset used to generate the simulated data, the results showed that the power estimates of the approximation formula are accurate for the true power of the robust-Wald test for the CHE model when there were a smaller number of categories ($C \leq 3$) and in cases where there were a larger number of categories ($C = 4$) and large datasets ($J \geq 60$). However, under some conditions, when there were more categories and a small to moderate total number of studies ($J \leq 48$), the approximation formula overestimated the true power by as much as $20.81$ percentage points. Furthermore, the balance of the number of studies across categories and the between-study heterogeneity impacted the magnitude of the power discrepancy, with imbalance in the number of studies across categories and smaller between-study heterogeneity resulting in the biggest power discrepancies. The approximation never underestimated true power. 

%Under the conditions I evaluated and the sampling techniques for the $k_j$ and $\sigma_j^2$, the approximation never underestimated the true power. Furthermore, across all conditions except for a few when $C=4$, the approximation power estimates were within the threshold of $5\%$ of the true power. However, I did not assess the mis-specification of these values. 

One possible reason that the approximation overestimated the true power could be related to the findings of my third aim, where I found that the simulated Type I error rates for the cluster-robust Wald test of the study-level categorical moderator are conservative when the number of contrasts ($q$) is greater and when the total number of studies was small. These findings are consistent with those of \textcite{tipton2015b} and \textcite{joshi_cluster_2022}; they also found that while none of the factors yielded Type I error rates substantially above nominal levels, the HTZ test has more conservative Type I error rates and becomes even more conservative when there are more contrasts. While the HTZ test performs well at controlling Type I error, the power of the test is penalized when the number of contrasts, $q$, increases. The reason is that as $q$ increases, the multiplier of the $Q$ to equate it to an $F$ ($[(\eta_z - q +1)/(\eta_zq)]$) and the denominator degrees of freedom of the $F$-statistic ($d_2 = \eta_z - q +1$) decrease \autocite{tipton2015b}. 

While this conservatism in the Type I error rate lowers the likelihood of making a Type I error, its source could also lower statistical power. The low power is attributable to the test being conservative, which could be why the approximation overstates the power. Perhaps the power of a more appropriately calibrated method for the Wald test, like the cluster wild bootstrap test (CWB) examined by \textcite{joshi_cluster_2022}, may be more aligned with the power estimates of the approximation.  
\textcite{joshi_cluster_2022} examined CWB as an alternative small-sample correction test to the HTZ test, and they found that CWB controls for Type I error and provides more power than the HTZ test, especially for tests of multiple contrasts. A future study validating the proposed approximation for the power of the CWB test is worth conducting, because the CWB is computationally intensive and it would be useful to be able to estimate power for a proposed analysis that involves CWB. 

%Something that can calculated in advanced. Do power calculation in advanced and look at the degrees of freedom, if very small than maybe don't trust the result.

In my results, I also found that the magnitude of the degrees of freedom is a useful diagnostic indicator for whether power approximations are accurate for the HTZ Wald test, where a small degrees of freedom and more contrasts could indicate that the power approximation will overestimate the true power and a researcher should then be more cautious about the associated power estimate (see Figure \ref{fig: df_mean}). However, if the number of contrasts is smaller ($q \leq 2$) or if the number of contrasts is greater ($q \geq 3$) and there is a larger degrees of freedom value ($d_2 \geq 12$), then the approximation will be reliable. When conducting an \textit{a priori} power analysis, a meta-analyst can look at the degrees of freedom to determine whether they can trust the results. Although it is important to note that a small degree of freedom does not guarantee a large discrepancy between actual and estimated power estimates. There were a large number of samples that had small degrees of freedom due to an imbalanced number of studies across categories, small between-study heterogeneity, four categories, and/or a small total number of studies, where the approximation was accurate. Additionally, the balance of the number of studies across categories and between-study heterogeneity also impacted the magnitude of the degrees of freedom. \textcite{tipton2015b} found that degrees of freedom in a meta-regression using HTZ were smaller for imbalanced covariates as well. Finally, if there is an interest in finding the power of the HTZ test for a larger number of contrasts and a smaller number of studies, then it may be necessary instead to simulate the power of the test as I did for this study. 

%Furthermore, as seen in Figure \ref{fig: df_mean}, all of the samples that had discrepancies greater than five percentage points had small degrees of freedom ($d_2 \leq 11.64$) and a higher number of categories ($C \geq 3$).

%For cases when there is more contrasts and smaller number of studies, it may be necessary to make power simulation for the cluster-robust Wald test for the CHE model using HTZ degrees of freedom more available.


Regarding the findings of my third aim independently, I was able to replicate the findings of \textcite{joshi_cluster_2022} and \textcite{tipton2015b} through a distinctly different way of simulating the design matrices and vector of $\beta$ meta-regression coefficients. Both studies used different covariate specifications that were more limited in scope. The design matrix that \textcite{tipton2015b} used in their simulation study had five covariates of various types, including one binary study-level moderator with extreme imbalance in the number of studies across the categories. They evaluated a different number of contrasts, which resulted in 26 unique coefficient vectors. In their first study, \textcite{joshi_cluster_2022} evaluated the HTZ test with the same design matrix as \textcite{tipton2015b} and 11 unique coefficient vectors. For their second study, \textcite{joshi_cluster_2022} generated a single categorical covariate as I did, but its value varied either at the study-level or effect-size level. The categorical covariate had either 3, 4, or 5 categories. For the $\beta$ coefficients, they fixed the intercept to 0.3 and varied only the first slope with three values, which resulted in 24 unique coefficient vectors. For my study, the design matrices only included study-level categorical moderators, and I induced imbalance in the number of studies across the categories through fixed proportions depending on the number of categories. Also, by generating the $\mu_c$ values through data-generating conditions, I evaluated $7,203$ unique coefficient vectors (See Figure \ref{fig:max_mu} for the maximum $\beta$ estimate value of each coefficient vector). I was also able to replicate the findings of \textcite{joshi_cluster_2022} and \textcite{tipton2015b} using a different working model than they used in their simulation studies.  \textcite{joshi_cluster_2022} used a correlated effects working model where \textcite{tipton2015b} used a fixed effects working model. I replicated their findings under a wider range of design matrices, regression coefficients, and a more general working model.   

%Other factors that impacted the degree to which the HTZ test was below the nominal error rate were the balance of the number of studies per category and the magnitude of between-heterogeneity. 


%% really can't rely on stylized in all conditions. It was always off. Similar pattern. max 2-3 % off from empirical in the conditions.  

For the conclusions of my second aim, I found that the approximate power estimates that result from sampling within-study characteristics from a stylized distribution followed the same pattern as those that result from sampling within-study characteristics from an empirical distribution. Neither sampling method performed well when $C=4$, and there was a small to moderate number of studies. However, the empirical sampling method performed slightly better with a maximum overestimation of $20.82$ percentage points compared to that of the stylized method's $25.72$ percentage point overestimation. Also, when the number of contrasts was small, the stylized method did overestimate the true power by more than five percentage points in some conditions ($C = 3$ and $J \leq 36$) by as much as $8.85$ percentage points. As \textcite{vembye2023} noted, since reliable pilot data is not always available for an \textit{a priori} power analysis, meta-analysts can use stylized distributions of $k_j$ and $\sigma_j^2$ for the approximation. In practice, I suggest factoring in that the stylized sampling method will overestimate the true power more than reliable pilot data. 

For the balanced assumption method, the true power was at most $25.3$ percentage points above and $24.2$ percentage points below a power of $60\%$. Assuming that the primary study characteristics are balanced resulted in substantially underestimated power estimates for the Wald test with the CHE+RVE model across all conditions except when $C=4$, and there were a small to moderate number of studies where it also overestimated the true power. Therefore, I do not recommend that meta-analysts use the balanced method in practice. 


\section{Limitations and Future Directions}

Below, I detail limitations to this study that should be considered to determine the extent to which these results can be generalized. I also highlight areas where this study can be extended. 

The proposed approximation of the cluster-robust Wald test only applies to study-level categorical moderators. I first focused on categorical moderators because they are commonly used in meta-analytic practice \autocite{ahn2012, tipton2019}. 
Additional work is needed to develop approximations for within-study categorical and between-study and within-study continuous moderators. As \textcite{vembye2023} noted, and I also faced in this dissertation study, it is necessary to make assumptions about the distribution of the covariate across studies and effect sizes. That can be challenging in practice. In future research, the balance of the number of effect sizes and the number of studies across covariates should also be considered for within-study moderators.  
The proposed power approximation for the robust-Wald test of study-level categorical moderators was developed for data structures that follow the CHE model, while \textcite{vembye2023} developed and evaluated approximations for the CE and MLMA working models for the test of the overall pooled effect as well. For my purposes, I chose not to look at special cases of the CHE model where there is no within-study variance or no correlation among effect sizes because meta-analytic data often has both sources of dependence \autocite{pustejovsky2022}. Because it is possible for data structures to have only one type of dependence, though, it may be worthwhile to develop approximations for these special cases. Additionally, the proposed approximation should be validated when the simulated data follows a different working model (such as $\rho=0$ for the MLMA model or $\omega=0$ for the CE model).

Furthermore, this approximation only proposed HTZ-based degrees of freedom, not model-based degrees of freedom, nor other robust tests such as the Eigen-decomposition F-test or the Eigen-decomposition and transformation test \autocite{tipton2015b}. I decided to first focus on developing the HTZ-based degrees of freedom for RVE because \textcite{vembye2023} found that the approximations that used RVE were more accurate than the model-based ones, where model-based tests had inflated Type I error for the test of the overall pooled effect. Also, the model-based degrees of freedom will have close to-correct Type I error only when the working model is correctly specified \autocite{vembye2023}. For the eigen-decomposition-based methods, \textcite{tipton2015b} found that they also resulted in high Type I error rates. 

% Also, there is still the open question of how the power and Type I error of the robust Wald tests compare to the power of the model-based Wald tests. Model-based tests could have inflated Type I error, as \textcite{vembye2023} found for the test of the overall pooled effect.

Additionally, for the simulated data, I assumed that the effect sizes in each study and across studies were equally correlated, which may not accurately reflect the reality in which the sampling correlation varies across studies. Also, it could be that variability in sampling correlation impacts the power of the true estimates, and therefore, also affects the discrepancy between the true and approximated power. Future studies could generate a sampling correlation that varies between studies by assuming it follows a $Beta$ distribution as was done in \textcite{tipton2015b, joshi_cluster_2022, vembye2023}. Another next step would be to examine when the sampling correlation used in the data-generating process is different from the sampling correlation used in the estimation or the approximation formula.

For this study, I only used one empirical distribution of $k_j$ and $\sigma_j^2$ to validate the approximation. Further steps should be taken to validate the approximation with other distributions of $k_j$ and $\sigma_j^2$ to test whether the approximation results in more inaccurate power estimates.  Another limitation of this study is that the distributions of $k_j$ and $\sigma_j^2$ for the stylized sampling method were pretty similar to those of the empirical distributions. Further work is needed to evaluate how robust the approximation is to using estimates of $k_j$ and $\sigma_j^2$ drawn from a distribution quite different from that of the data-generating sample. 

Further work is needed to evaluate imputing other numbers besides the mean of the empirical distribution for the balanced method for assuming values for $\sigma_j^2$ an $k_j$ in the approximation. Alternatives include using the harmonic mean of $\sigma_j^2$, which will lead to somewhat larger studies overall, or computing the weights for an entire empirical distribution given the design characteristics $\tau^2$, $\omega^2$, and $\rho$ and imputing the average of the weights. 

The conclusions of this study are limited to the study design conditions examined (Table \ref{tab:experimentalconditions}). For example, the smallest value for the total number of studies factor that I evaluated was $24$. Still, many applied researchers have meta-analytic datasets with an even smaller total number of studies. Furthermore, I only looked at a maximum of four categories when a categorical moderator could have many more categories in practice. Also, because there are infinite possibilities, specifying alternative hypotheses for multiple contrasts can be challenging. The patterns of the $\mu_c$ were my first attempt to specify possible alternative hypotheses, but this can be refined in future studies. I believe a review of meta-analyses on the number of categories, the number of studies per category, and the distribution of regression slopes is needed to create more design conditions to test this approximation. 

%Furthermore, applied meta-analysts often encounter missing data in moderators across primary studies, decreasing the meta-analysis's power. Future studies can look at ways to account for missing data in the power approximation. 

I simulated standardized mean differences, so given distributional similarities, these results likely can also be applied to Fisher-Z transformed correlation effect sizes. Still, the results will not translate to other effect size types like odds ratio without further assumptions \autocite{vembye2023}.

Finally, further work is needed to develop guidelines for conducting a power analysis using this approximation, as was done in \textcite{vembye2024}. Also, to make the approximation more accessible, it should be implemented in an R software package such as \texttt{POMADE} \autocite{POMADE}.


\section{Conclusions}

An \textit{a priori} power analysis helps meta-analysts and potential funders determine whether a number of studies is large enough to detect an effect size of practical importance. Additionally, \textit{a priori} power analysis helps in planning a confirmatory research project when developing the analytic methodology. Applied researchers could conduct a power analysis for a meta-regression through a Monte Carlo simulation. However, such an analysis is not always accessible and takes time to develop. Having the approximation formula available makes the \textit{a priori} power analysis more straightforward.


%Unlike \textcite{vembye2023}, we did not find that low amount of $\tau$ to be the primary reason for overestimating the true power for stylized...while $\tau$ did lead to the greater magnitude of the discrepancies. 

%I have the following recommendations if using the approximation for the HTZ test with CHE+RVE. 

%Considering that the approximation never underestimated the true power when using pilot data or stylized distributions for the primary study characteristics, under the conditions I evaluated, suggests that using this approximation will never lead an applied researcher to conclude erroneously that a prospective study is not worth doing due to low power. 



% The figure shows that small values of $d_2$ in combination with the number of categories result in discrepancies of more than five percentage points. The range of $d_2$ values that resulted in power estimate discrepancies of more than five percentage points is $(3.51, 11.64)$, and almost all have four categories (except for the two samples I mentioned earlier from pattern 3b). 



%My main impression from the figure is that the approximations can get less accurate (overstating power) when the degrees of freedom are very small. So perhaps d.f. is a useful diagnostic indicator for whether power approximations are accurate?\textbf{}

